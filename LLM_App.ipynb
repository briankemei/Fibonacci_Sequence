{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4tv5CHX8jZK78+2ylYLio",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briankemei/Fibonacci_Sequence/blob/main/LLM_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "UMlapWa13E9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYqZJryg3A2L"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI model\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DCkV1nx4kuj",
        "outputId": "a883aa25-93cf-4556-c30b-b9791bb5dae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Action to be\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#Define the prompt template\n",
        "messages = [\n",
        "    SystemMessage(content= \"Translate the following into swahili \"),\n",
        "    HumanMessage(content= \"Go to the shop and buy food\"),\n",
        "    ]\n",
        "\n",
        "#Define the parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "#Chain components together\n",
        "chain =  model | parser\n",
        "\n",
        "#result = model.invoke(messages)\n",
        "#parse( model.invoke(messages))\n",
        "\n",
        "#Invoke the chain with input\n",
        "chain.invoke(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dbN4zUnWJarE",
        "outputId": "323548fd-12c9-4466-e387-52353c7e345b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nenda dukani ukanunue chakula'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Step 1: Define the prompt template\n",
        "system_template = \"Translate the following into swhaili:\"\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=system_template),\n",
        "    HumanMessage(content=\"Hello\")\n",
        "])\n",
        "\n",
        "# Step 2: Invoke the prompt with inputs\n",
        "result = prompt_template.invoke({\"\": \"italian\", \"text\": \"hi\"})\n",
        "\n",
        "\n",
        "# Step 3: Check the result as messages (optional)\n",
        "result.to_messages()\n",
        "\n",
        "# Step 4: Define the model and parser, then create the chain\n",
        " # Set the temperature as needed\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Set up the chain\n",
        "chain = prompt_template | model | parser\n",
        "\n",
        "# Step 5: Invoke the chain with input\n",
        "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
        "# Step 6: Display the final output\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QZsZHYt5HPUt",
        "outputId": "4e2e3f75-921b-4787-cb95-cb794eea3e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Habari'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "SystemMessage ='Translate the following into italian:'\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"Hello\")]\n",
        ")\n",
        "result = prompt_template.invoke({\"English\": \"italian\", \"text\": \"Welcome\"})\n",
        "\n",
        "result\n",
        "#ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')])\n",
        "\n",
        "result.to_messages()\n",
        "#[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')]\n",
        "chain = prompt_template\n",
        "chain.invoke({\"English\": \"italian\", \"text\": \"hi\"})\n",
        "\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot5oVkFE714p",
        "outputId": "72734960-0f9e-4667-ecb4-7ecbeaaba9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}